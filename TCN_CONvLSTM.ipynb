{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK5F6S2h8p6eciJson2DaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toonzzzrock/project/blob/main/TCN_CONvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import"
      ],
      "metadata": {
        "id": "5PmTsV6hJnnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0agDeHoNHzQo"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras-tcn --no-dependencies\n",
        "from tcn import TCN, tcn_full_summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install time-series-generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU9fbDFcJ2vC",
        "outputId": "d45867f8-c98f-48ba-e152-4f789598014b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting time-series-generator\n",
            "  Downloading time_series_generator-0.2.8-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from time-series-generator) (1.21.6)\n",
            "Installing collected packages: time-series-generator\n",
            "Successfully installed time-series-generator-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import Model, Input, Sequential\n",
        "from tensorflow.keras.layers import Dense, Masking, LSTM, Embedding, Dropout, ConvLSTM2D, MaxPooling3D, BatchNormalization, TimeDistributed, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.activations import gelu\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError as rmse\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "90vewQ27IBsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time_series_generator import TimeseriesGenerator as datagen"
      ],
      "metadata": {
        "id": "4sCvhi4eJ3xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "2bQB53dbItId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import re\n",
        "import requests\n",
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "x0aNnrcbIVAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unmlMB0cIN47",
        "outputId": "4bbabd9c-4444-40aa-f683-04e37c6b43b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Stock_data = pd.read_parquet(r'/content/gdrive/MyDrive/Fund/X_train_stock.parquet.gzip')  \n",
        "News_data = pd.read_parquet(r'/content/gdrive/MyDrive/Fund/News_data_clean.parquet.gzip')  "
      ],
      "metadata": {
        "id": "imNTBMHGIYdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean data"
      ],
      "metadata": {
        "id": "xWl9YCXJJum8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 20000\n",
        "embedding_vecor_length = 256\n",
        "#max_review_length = 800\n",
        "input_length = 30\n",
        "output_length = 5"
      ],
      "metadata": {
        "id": "fqhagRgCIDkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "News"
      ],
      "metadata": {
        "id": "txlI_4lmKhEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=num_words,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ')\n",
        "tokenizer.fit_on_texts(News_data['all_text'])"
      ],
      "metadata": {
        "id": "nPl9cjjoIzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_News_token = tokenizer.texts_to_sequences(News_data['all_text'])"
      ],
      "metadata": {
        "id": "4UtBVdawI3E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# truncate and pad input sequences\n",
        "X_News_train = sequence.pad_sequences(X_News_token)"
      ],
      "metadata": {
        "id": "CUfSXkTBI5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_News_train.shape"
      ],
      "metadata": {
        "id": "hujxeKDoO5vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stock"
      ],
      "metadata": {
        "id": "2n3N8FEPKin6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stock_data = Stock_data.fillna(1)"
      ],
      "metadata": {
        "id": "PkG4krQsLmPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stock_data = Stock_data.drop(columns = ['date'])"
      ],
      "metadata": {
        "id": "nqMGersZLSP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stock_data2 = np.log(Stock_data / Stock_data.shift(1))"
      ],
      "metadata": {
        "id": "BhF8TyxhK7o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stock_data2 = Stock_data2.fillna(0)"
      ],
      "metadata": {
        "id": "TnQMexuWLXZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data generator"
      ],
      "metadata": {
        "id": "dpkV5t4gMTlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all = pd.concat([X_News_token, Stock_data2], axis = 1).to_numpy()"
      ],
      "metadata": {
        "id": "B8i0hZrhMX9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_dataset = datagen(X_train, Stock_data2['Close'], length=input_length, length_output=output_length, batch_size=128, sampling_rate = 5)"
      ],
      "metadata": {
        "id": "6k_Eg4gBKVXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "jrgwUTBrKWTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def My_ConvLSTM_Model(frames, channels, stock_indicator, max_review_length, predict_frame):\n",
        "  \n",
        "    trailer_input  = [Input(shape=(frames, channels, max_review_length), name='News'),\n",
        "                      Input(shape=(frames, stock_indicator), name='Stock movement')\n",
        "    ]\n",
        "\n",
        "    \n",
        "    Embedding_layer = Embedding(num_words, \n",
        "                             embedding_vecor_length, \n",
        "                             input_length=max_review_length,\n",
        "                             trainable=False,\n",
        "                             mask_zero=True,\n",
        "                             name = 'News_embedding_layer'\n",
        "    )(trailer_input[0])\n",
        "\n",
        "    Masking_layer = Masking(mask_value=0.0, name = 'Masking_News_layer')(Embedding_layer)\n",
        "\n",
        "    first_ConvLSTM = ConvLSTM2D(filters=20, kernel_size=(16, 16)\n",
        "                       , data_format='channels_first'\n",
        "                       , recurrent_activation='hard_sigmoid'\n",
        "                       , activation='tanh'\n",
        "                       , padding='same', return_sequences=True)(Masking_layer)\n",
        "\n",
        "    first_BatchNormalization = BatchNormalization()(first_ConvLSTM)\n",
        "    first_Pooling = MaxPooling3D(pool_size=(1, 8, 4), padding='same', data_format='channels_first')(first_BatchNormalization)\n",
        "    \n",
        "    first_ConvLSTM = ConvLSTM2D(filters=10, kernel_size=(16, 16)\n",
        "                        , data_format='channels_first'\n",
        "                        , recurrent_activation='hard_sigmoid'\n",
        "                       , activation='tanh'\n",
        "                       , padding='same', return_sequences=True)(first_Pooling)\n",
        "    first_BatchNormalization = BatchNormalization()(first_ConvLSTM)\n",
        "    first_Pooling = MaxPooling3D(pool_size=(1, 4, 4), padding='same', data_format='channels_first')(first_BatchNormalization)\n",
        "\n",
        "    branch_ConvLSTM = ConvLSTM2D(filters=5, kernel_size=(8, 8)\n",
        "                        , data_format='channels_first'\n",
        "                        , stateful = False\n",
        "                        , kernel_initializer='random_uniform'\n",
        "                        , padding='same', return_sequences=True)(first_Pooling)\n",
        "    branch_Pooling = MaxPooling3D(pool_size=(1, 4, 2), padding='same', data_format='channels_first')(branch_ConvLSTM)\n",
        "\n",
        "    flat_layer = TimeDistributed(Flatten())(branch_Pooling)\n",
        "    target = TimeDistributed(Dense(256))(flat_layer)\n",
        "    target = TimeDistributed(Dense(64))(target)\n",
        "    target = TimeDistributed(Dense(16))(target)\n",
        "\n",
        "    flat = Flatten()(target)\n",
        "    flat_BatchNormalization = BatchNormalization()(flat)\n",
        "    first_dropout = Dropout(0.25)(flat_BatchNormalization)\n",
        "    \n",
        "    dense_layer = Dense(256, activation=gelu)(first_dropout)\n",
        "    flat_BatchNormalization = BatchNormalization()(dense_layer)\n",
        "    first_dropout = Dropout(0.4)(flat_BatchNormalization)\n",
        "    \n",
        "    dense_layer = Dense(64, activation=gelu)(first_dropout)\n",
        "    flat_BatchNormalization = BatchNormalization()(dense_layer)\n",
        "    News_output = Dropout(0.4)(flat_BatchNormalization)\n",
        "    #-----------------------------\n",
        "\n",
        "    TCN_layer = TCN(input_shape=(frames, stock_indicator), nb_filters=128, return_sequences=True, dilations=[1, 2, 4, 8, 16, 32])(trailer_input[1])\n",
        "    TCN_layer2 = TCN(nb_filters=64, return_sequences=True, dilations=[1, 2, 4, 8, 16])(TCN_layer)\n",
        "\n",
        "    flat = Flatten()(TCN_layer2)\n",
        "    Stock_dense_layer = Dense(64, activation=gelu)(flat)\n",
        "    Stock_BatchNormalization = BatchNormalization()(Stock_dense_layer)\n",
        "    Stock_output = Dropout(0.4)(Stock_BatchNormalization)\n",
        "    \n",
        "    concat_layers = tf.concat([News_output, Stock_output], 0)\n",
        "    outputs = Dense(predict_frame, activation='linear')(concat_layers)\n",
        "    \n",
        "    seq = Model(inputs=trailer_input, outputs=outputs, name='Model ')\n",
        "    \n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "hGeUfKQmIEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_ConvLSTM_Model(frames = 60, channels = 1, stock_indicator = 10, max_review_length = 1024, predict_frame = 5)"
      ],
      "metadata": {
        "id": "tHJNy5AkIF3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "WrWn8l0uIGwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "5FHvmqABIIoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "Bks61fkHKZDU"
      }
    }
  ]
}